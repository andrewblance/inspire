{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "start_time = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import scipy\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.util import ngrams\n",
    "import numpy as np\n",
    "import re\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split Incoming Enteries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_text(text):\n",
    "    entries = []\n",
    "    for i in range(len(text)):\n",
    "        text_split = re.split(\"ENDTITLES|ENDTITLE|ENDABSTRACTS\",text[i])\n",
    "        title1 = text_split[0].strip()\n",
    "        title2 = text_split[1].strip()\n",
    "        abstract = text_split[2].replace(\"\\r\\n\",\" \").strip()\n",
    "        decision = text_split[3].strip()\n",
    "        entries.append({u'title':title1,u'abstract':abstract,u'decision':decision})\n",
    "    return(entries)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create n-grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def BIGRAMS(string):\n",
    "    token = nltk.word_tokenize(string)\n",
    "    bigram = ngrams(token,2)\n",
    "    bigrams = ', '.join(' '.join((a, b)) for a, b in bigram)\n",
    "    biNoSpace = bigrams.replace(\" \",\"\")\n",
    "    BIGRAM = biNoSpace.replace(\",\", \" \")\n",
    "    return BIGRAM\n",
    "    \n",
    "def TRIGRAMS(string):\n",
    "    token = nltk.word_tokenize(string)\n",
    "    trigram = ngrams(token, 3)\n",
    "    trigrams = ', '.join(' '.join((a, b, c)) for a, b, c in trigram)\n",
    "    triNoSpace = trigrams.replace(\" \",\"\")\n",
    "    TRIGRAM = triNoSpace.replace(\",\", \" \")\n",
    "    return TRIGRAM\n",
    "\n",
    "def QUADGRAMS(string):\n",
    "    token = nltk.word_tokenize(string)\n",
    "    quadgram = ngrams(token, 4)\n",
    "    quadgrams = ', '.join(' '.join((a, b, c, d)) for a, b, c, d in quadgram)\n",
    "    quadNoSpace = quadgrams.replace(\" \",\"\")\n",
    "    QUADGRAM = quadNoSpace.replace(\",\", \" \")  \n",
    "    return QUADGRAM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load any csv's"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load(name):\n",
    "    with open(name+\".csv\", \"rb\") as f:\n",
    "        File = np.loadtxt(f, dtype=str)\n",
    "    return File"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function to Search:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Search(Abs, W):\n",
    "    ListOfWords = []\n",
    "    match = set(Abs).intersection(set(W))\n",
    "    Match = list(match)\n",
    "    i=0\n",
    "    while (i < len(Match)):\n",
    "        ListOfWords.append(Match[i])\n",
    "        #print(Match[i])\n",
    "        i = i + 1  \n",
    "    return ListOfWords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "thing to tie it all together:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Key_Num(TEXT, DICT):\n",
    "    S1=Search(TEXT.split()             ,DICT)\n",
    "    S2=Search(BIGRAMS(TEXT).split()    ,DICT)\n",
    "    S3=Search(TRIGRAMS(TEXT).split()   ,DICT)\n",
    "    S4=Search(QUADGRAMS(TEXT).split()  ,DICT)\n",
    "\n",
    "    score1 = len(S1)\n",
    "    score2 = len(S2)\n",
    "    score3 = len(S3)\n",
    "    score4 = len(S4)\n",
    "    total =sum([score1,score2,score3,score4])\n",
    "    #print(\"The amount of terms found is {0}\" .format(score))\n",
    "    return [score1, score2, score3, score4,total ]\n",
    "\n",
    "#print(Key_Num(TEXT[1].get('abstract').lower(),key))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test with abstract"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "text1 = \"Cellular non-nonlinear network model of microbial fuel cellENDTITLECellular non-nonlinear network model of microbial fuel cell ENDTITLES A cellular non-linear network (CNN) is a uniform regular array of locally\\r\\nconnected continuous-state machines, or nodes, which update their states\\r\\nsimultaneously in discrete time. A microbial fuel cell (MFC) is an\\r\\nelectro-chemical reactor using the metabolism of bacteria to drive an\\r\\nelectrical current. In a CNN model of the MFC, each node takes a vector of\\r\\nstates which represent geometrical characteristics of the cell, like the\\r\\nelectrodes or impermeable borders, and quantify measurable properties like\\r\\nbacterial population, charges produced and hydrogen ions concentrations. The\\r\\nmodel allows the study of integral reaction of the MFC, including temporal\\r\\noutputs, to spatial disturbances of the bacterial population and supply of\\r\\nnutrients. The model can also be used to evaluate inhomogeneous configurations\\r\\nof bacterial populations attached on the electrode biofilms. ENDABSTRACTS Rejected\"\n",
    "text2 = \"Electroweak and QCD corrections to Z-boson production with one b jet in a massive 5 Flavor Scheme ENDTITLE Electroweak and QCD corrections to Z-boson production with one b jet in a massive 5 Flavor Scheme ENDTITLES We compute the O(αsα2) and O(α2sα) contributions to the production cross section of a Z boson with one b jet at the Large Hadron Collider (LHC), and study their phenomenological relevance for LHC physics. The accurate prediction of hadronic Z+b-jet production is needed to control a background that greatly affects both the measurement of Higgs-boson properties and searches of new physics at the LHC. At the same time it could enable the first precise measurement of the b-quark parton distribution function. In this context b-quark mass effects become relevant and need to be studied with care, both at the level of the hard process and at the level of the initial- and final-state parton evolution. It is the aim of this paper to explore some of these issues in the framework of a massive 5 Flavor Scheme and to assess the need for both the inclusion of electroweak corrections, in addition to QCD corrections, and b-quark mass effects in the prediction of total and differential cross sections for hadronic Z+b-jet production. ENDABSTRACTS CORE\"\n",
    "text3=\"\"\"N2HDECAY: Higgs Boson Decays in the Different Phases of the N2HDMENDTITLE\n",
    "    N2HDECAY: Higgs Boson Decays in the Different Phases of the N2HDMENDTITLES\n",
    "    The program N2HDECAY calculates the branching ratios and decay widths of the Higgs bosons of the Next-to-Two-Higgs-Doublet Model (N2HDM). The code incorporates the dominant higher-order effects by including QCD corrections and off-shell decay modes. The N2HDM is an extension of the Standard Model by a Higgs doublet and a real Higgs singlet. Its phenomenology can change dramatically depending on which global symmetries are broken by electroweak symmetry breaking. It can feature a large visible Higgs sector in the broken phase, behave like a 2HDM with scalar singlet dark matter in the dark singlet phase, or, in the inert doublet phase, extend an inert doublet model by mixing a singlet with the SM Higgs boson. N2HDECAY provides precise predictions for the decays of the Higgs bosons in all of these phases.ENDABSTRACTS\n",
    "    CORE\"\"\"\n",
    "text = [text1,text2,text3]\n",
    "TEXT     = split_text(text)\n",
    "TITLE= []\n",
    "ABSTRACT =[]\n",
    "DECISION =[]\n",
    "for i in range(len(text)):\n",
    "    TITLE.append(TEXT[i].get('title').lower())\n",
    "    ABSTRACT.append(TEXT[i].get('abstract').lower())\n",
    "    DECISION.append(TEXT[i].get('decision').lower())\n",
    "\n",
    "key = load(\"KeyWords\")\n",
    "q=0\n",
    "while (q < len(key)):\n",
    "    key[q] = key[q].lower()\n",
    "    q = q + 1\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "RESULTS = [] \n",
    "for i in range(len(text)):\n",
    "    RESULTS.append(Key_Num(TITLE[i], key))\n",
    "    #ListOfWords = []\n",
    "    RESULTS.append(Key_Num(ABSTRACT[i], key))\n",
    "    #ListOfWords=[]\n",
    "    RESULTS.append(DECISION[i])\n",
    "\n",
    "for n, i in enumerate(RESULTS):\n",
    "    if i == \"rejected\" :\n",
    "        RESULTS[n] = [0]\n",
    "    if i == \"core\" :\n",
    "        RESULTS[n] = [2]\n",
    "    if i == \"non-core\" :\n",
    "        RESULTS[n] = [1]\n",
    "\n",
    "RESULT_array=np.reshape(np.hstack(RESULTS),(len(text),11))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "My program took 3.290325880050659 to run\n"
     ]
    }
   ],
   "source": [
    "print(\"My program took\", time.time() - start_time, \"to run\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save file with column headings: \n",
    "#Title:words, Title:bigrams, Title:trigrams, Title:quadgrams, Title:total, Abstract:words, Abstract:bigrams, Abstract:trigrams, Abstract:quadgrams, Abstract:total, classification\n",
    "def output_file(name,output):\n",
    "    with open(name+\".pkl\",'wb') as f:\n",
    "        pickle.dump(output,f)\n",
    "output_file('keyword_occurances',RESULT_array)            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 2  0  0  0  2  7  0  0  0  7  0]\n",
      " [ 5  0  0  0  5 13  3  1  0 17  2]\n",
      " [ 2  0  0  0  2 12  5  1  1 19  2]]\n"
     ]
    }
   ],
   "source": [
    "with open('keyword_occurances.pkl','rb') as f:\n",
    "    mydata= pickle.load(f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
