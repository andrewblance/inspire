{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "start_time = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import scipy\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.util import ngrams\n",
    "import numpy as np\n",
    "import re\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split Incoming Enteries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "ETITLE = [\"ENDTITLE\"]\n",
    "def split_text(text):\n",
    "    entries = []\n",
    "    j=0\n",
    "    while (j <(len(text))):\n",
    "        if (text[j].count('ENDTITLE') == 3):\n",
    "            text_split = re.split(\"ENDTITLE|ENDTITLE|ENDTITLES|ENDABSTRACTS\",text[j])\n",
    "            title1 = text_split[0].strip()\n",
    "            title2 = text_split[1].strip()\n",
    "            title3 = text_split[2].strip()\n",
    "            abstract = text_split[3].replace(\"\\r\\n\",\" \").strip()\n",
    "            decision = text_split[4].strip()\n",
    "            entries.append({u'title':title1,u'abstract':abstract,u'decision':decision})\n",
    "            j = j +1\n",
    "        elif (set(ETITLE).intersection(text[j].split()) == set(['ENDTITLE'])):\n",
    "            text_split = re.split(\"ENDTITLES|ENDTITLE|ENDABSTRACTS\",text[j])\n",
    "            title1 = text_split[0].strip()\n",
    "            title2 = text_split[1].strip()\n",
    "            abstract = text_split[2].replace(\"\\r\\n\",\" \").strip()\n",
    "            decision = text_split[3].strip()\n",
    "            entries.append({u'title':title1,u'abstract':abstract,u'decision':decision})\n",
    "            j = j +1\n",
    "        else: \n",
    "            text_split = re.split(\"ENDTITLES|ENDABSTRACTS\",text[j])\n",
    "            title = text_split[0].strip()\n",
    "            abstract = text_split[1].replace(\"\\r\\n\",\" \").strip()\n",
    "            decision = text_split[2].strip()\n",
    "            entries.append({u'title':title,u'abstract':abstract,u'decision':decision})\n",
    "            j = j + 1\n",
    "    return(entries)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create n-grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def BIGRAMS(string):\n",
    "    token = nltk.word_tokenize(string)\n",
    "    bigram = ngrams(token,2)\n",
    "    bigrams = ', '.join(' '.join((a, b)) for a, b in bigram)\n",
    "    biNoSpace = bigrams.replace(\" \",\"\")\n",
    "    BIGRAM = biNoSpace.replace(\",\", \" \")\n",
    "    return BIGRAM\n",
    "    \n",
    "def TRIGRAMS(string):\n",
    "    token = nltk.word_tokenize(string)\n",
    "    trigram = ngrams(token, 3)\n",
    "    trigrams = ', '.join(' '.join((a, b, c)) for a, b, c in trigram)\n",
    "    triNoSpace = trigrams.replace(\" \",\"\")\n",
    "    TRIGRAM = triNoSpace.replace(\",\", \" \")\n",
    "    return TRIGRAM\n",
    "\n",
    "def QUADGRAMS(string):\n",
    "    token = nltk.word_tokenize(string)\n",
    "    quadgram = ngrams(token, 4)\n",
    "    quadgrams = ', '.join(' '.join((a, b, c, d)) for a, b, c, d in quadgram)\n",
    "    quadNoSpace = quadgrams.replace(\" \",\"\")\n",
    "    QUADGRAM = quadNoSpace.replace(\",\", \" \")  \n",
    "    return QUADGRAM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load any csv's"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load(name):\n",
    "    with open(name+\".csv\", \"rb\") as f:\n",
    "        File = np.loadtxt(f, dtype=str)\n",
    "    return File"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function to Search:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Search(Abs, W):\n",
    "    ListOfWords = []\n",
    "    match = set(Abs).intersection(set(W))\n",
    "    Match = list(match)\n",
    "    i=0\n",
    "    while (i < len(Match)):\n",
    "        ListOfWords.append(Match[i])\n",
    "        i = i + 1\n",
    "    return ListOfWords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "thing to tie it all together:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Key_Num(TEXT, DICT):\n",
    "    S1 = Search(TEXT.split()             ,DICT)\n",
    "    S2 = Search(BIGRAMS(TEXT).split()    ,DICT)\n",
    "    S3 = Search(TRIGRAMS(TEXT).split()   ,DICT)\n",
    "    S4 = Search(QUADGRAMS(TEXT).split()  ,DICT)\n",
    "    \n",
    "    score1 = len(S1)\n",
    "    score2 = len(S2)\n",
    "    score3 = len(S3)\n",
    "    score4 = len(S4)\n",
    "    total = sum([score1, score2, score3 , score4])\n",
    "    \n",
    "    return [score1, score2, score3, score4, total]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "function to import data, and function to remove non ascii characters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _removeNonAscii(s): return \"\".join(i for i in s if ord(i)<128)\n",
    "\n",
    "Data = []\n",
    "with open(\"small.txt\", \"r\") as ins:\n",
    "    for line in ins:\n",
    "        line = _removeNonAscii(line)\n",
    "        line = line.replace(\",\", \"\")\n",
    "        line = line.replace(\".\", \"\")\n",
    "        Data.append(line)      "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets run it!!!!!!!! (gets messier after this, maybe clean it up later?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEXT = split_text(Data)\n",
    "\n",
    "TITLE = []\n",
    "ABSTRACT = []\n",
    "DECISION = []\n",
    "\n",
    "for r in range(len(Data)):\n",
    "    TITLE.append(TEXT[r].get('title').lower())\n",
    "    ABSTRACT.append(TEXT[r].get('abstract').lower())\n",
    "    DECISION.append(TEXT[r].get('decision').lower())\n",
    "\n",
    "key = load(\"KeyWords\")\n",
    "q=0\n",
    "while (q < len(key)):\n",
    "    key[q] = key[q].lower()\n",
    "    q = q + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "RESULTS = [] \n",
    "for i in range(len(Data)):\n",
    "    RESULTS.append(Key_Num(TITLE[i], key))\n",
    "    RESULTS.append(Key_Num(ABSTRACT[i], key))\n",
    "    RESULTS.append(DECISION[i])\n",
    "\n",
    "for n, i in enumerate(RESULTS):\n",
    "    if i == \"rejected\" :\n",
    "        RESULTS[n] = 0\n",
    "    if i == \"core\" :\n",
    "        RESULTS[n] = 2\n",
    "    if i == \"non-core\" :\n",
    "        RESULTS[n] = 1\n",
    "\n",
    "RESULT_array=np.reshape(np.hstack(RESULTS), (len(Data),11))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "not every entry tell us if it is core, so it sucks and we can get rid of it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "RESULTS_ARRAY = []\n",
    "m = 0\n",
    "while (m < len(RESULT_array)):\n",
    "    if (RESULT_array[m][10] != ''):\n",
    "        RESULTS_ARRAY.append(RESULT_array[m])\n",
    "    m = m + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "function to save everything:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def output_file(name,output):\n",
    "    with open(name+\".pkl\",'wb') as f:\n",
    "        pickle.dump(output,f)\n",
    "        \n",
    "output_file('keyword_occurances',RESULTS_ARRAY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('keyword_occurances.pkl','rb') as f:\n",
    "    mydata= pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('My program took', 109.19884300231934, 'to run')\n"
     ]
    }
   ],
   "source": [
    "print(\"My program took\", time.time() - start_time, \"to run\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
